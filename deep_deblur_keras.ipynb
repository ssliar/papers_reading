{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep_deblur_keras\n",
    "* [Deep Generative Filter for motion deblurring](https://arxiv.org/pdf/1709.03481.pdf)\n",
    "*conference:ICCV2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as gb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization x to [-1,1]\n",
    "def normalization(x):\n",
    "    return x / 127.5 - 1\n",
    "# according the image path to read the image and covert it\n",
    "# to the given size, then slice it, finally return the full and blur images\n",
    "def format_image(image_path, size):\n",
    "    image = Image.open(image_path)\n",
    "    # slice image into full and blur images\n",
    "    image_full = image.crop((0, 0, image.size[0] / 2, image.size[1]))\n",
    "    # Note the full image in left, the blur image in right\n",
    "    image_blur = image.crop((image.size[0] / 2, 0, image.size[0], image.size[1]))\n",
    "    # image_full.show()\n",
    "    # image_blur.show()\n",
    "    image_full = image_full.resize((size, size), Image.ANTIALIAS)\n",
    "    image_blur = image_blur.resize((size, size), Image.ANTIALIAS)\n",
    "    # return the numpy arrays\n",
    "    return np.array(image_full), np.array(image_blur)\n",
    "\n",
    "# convert images to hdf5 data\n",
    "def build_hdf5(jpeg_dir, size=256):\n",
    "    # put data in HDF5\n",
    "    hdf5_file = os.path.join('data', 'data.h5')\n",
    "    with h5py.File(hdf5_file, 'w') as f:\n",
    "        for data_type in tqdm(['train', 'test'], desc='create HDF5 dataset from images'):\n",
    "            data_path = jpeg_dir + '/%s/*.jpg' % data_type\n",
    "            images_path = gb.glob(data_path)\n",
    "            # print(images_path)\n",
    "            data_full = []\n",
    "            data_blur = []\n",
    "            for image_path in images_path:\n",
    "                image_full, image_blur = format_image(image_path, size)\n",
    "                data_full.append(image_full)\n",
    "                data_blur.append(image_blur)\n",
    "            # print(len(data_full))\n",
    "            # print(len(data_blur))\n",
    "            f.create_dataset('%s_data_full' % data_type, data=data_full)\n",
    "            f.create_dataset('%s_data_blur' % data_type, data=data_blur)\n",
    "\n",
    "# load data by data type\n",
    "def load_data(data_type):\n",
    "    with h5py.File('data/data.h5', 'r') as f:\n",
    "        data_full = f['%s_data_full' % data_type][:].astype(np.float32)\n",
    "        data_full = normalization(data_full)\n",
    "        data_blur = f['%s_data_blur' % data_type][:].astype(np.float32)\n",
    "        data_blur = normalization(data_blur)\n",
    "        return data_full, data_blur\n",
    "\n",
    "def generate_image(full, blur, generated, path, epoch=None, index=None):\n",
    "    full = full * 127.5 + 127.5\n",
    "    blur = blur * 127.5 + 127.5\n",
    "    generated = generated * 127.5 + 127.5\n",
    "    for i in range(generated.shape[0]):\n",
    "        image_full = full[i, :, :, :]\n",
    "        image_blur = blur[i, :, :, :]\n",
    "        image_generated = generated[i, :, :, :]\n",
    "        image = np.concatenate((image_full, image_blur, image_generated), axis=1)\n",
    "        if (epoch is not None) and (index is not None):\n",
    "            Image.fromarray(image.astype(np.uint8)).save(path + str(epoch + 1) + '_' + str(index + 1) + '.png')\n",
    "        else:\n",
    "            Image.fromarray(image.astype(np.uint8)).save(path + str(i) + '.png')\n",
    "# format_image('data/small/test/301.jpg', size=256)\n",
    "# build_hdf5('data/small')\n",
    "# img_full, img_blur = load_data('train')\n",
    "# print(img_full, '\\n', len(img_blur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.core import Dropout, Dense, Flatten, Lambda\n",
    "from keras.layers.merge import Average\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "# the paper defined hyper-parameter:chr\n",
    "channel_rate = 64\n",
    "# Note the image_shape must be multiple of patch_shape\n",
    "image_shape = (256, 256, 3)\n",
    "patch_shape = (channel_rate, channel_rate, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def dense_block(inputs, dilation_factor=None):\n",
    "    x = LeakyReLU(alpha=0.2)(inputs)\n",
    "    x = Convolution2D(filters=4 * channel_rate, kernel_size=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    # the 3 × 3 convolutions along the dense field are alternated between ‘spatial’ convolution\n",
    "    # and ‘dilated’ convolution with linearly increasing dilation factor\n",
    "    if dilation_factor is not None:\n",
    "        x = Convolution2D(filters=channel_rate, kernel_size=(3, 3), padding='same',\n",
    "                          dilation_rate=dilation_factor)(x)\n",
    "    else:\n",
    "        x = Convolution2D(filters=channel_rate, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # add Gaussian noise\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    return x\n",
    "\n",
    "def generator_model():\n",
    "    # Input Image, Note the shape is variable\n",
    "    inputs = Input(shape=(None, None, 3))\n",
    "    # The Head\n",
    "    h = Convolution2D(filters=4 * channel_rate, kernel_size=(3, 3), padding='same')(inputs)\n",
    "    # The Dense Field\n",
    "    d_1 = dense_block(inputs=h)\n",
    "    x = concatenate([h, d_1])\n",
    "    # the paper used dilated convolution at every even numbered layer within the dense field\n",
    "    d_2 = dense_block(inputs=x, dilation_factor=(1, 1))\n",
    "    x = concatenate([x, d_2])\n",
    "    d_3 = dense_block(inputs=x)\n",
    "    x = concatenate([x, d_3])\n",
    "    d_4 = dense_block(inputs=x, dilation_factor=(2, 2))\n",
    "    x = concatenate([x, d_4])\n",
    "    d_5 = dense_block(inputs=x)\n",
    "    x = concatenate([x, d_5])\n",
    "    d_6 = dense_block(inputs=x, dilation_factor=(3, 3))\n",
    "    x = concatenate([x, d_6])\n",
    "    d_7 = dense_block(inputs=x)\n",
    "    x = concatenate([x, d_7])\n",
    "    d_8 = dense_block(inputs=x, dilation_factor=(2, 2))\n",
    "    x = concatenate([x, d_8])\n",
    "    d_9 = dense_block(inputs=x)\n",
    "    x = concatenate([x, d_9])\n",
    "    d_10 = dense_block(inputs=x, dilation_factor=(1, 1))\n",
    "    # The Tail\n",
    "    x = LeakyReLU(alpha=0.2)(d_10)\n",
    "    x = Convolution2D(filters=4 * channel_rate, kernel_size=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # The Global Skip Connection\n",
    "    x = concatenate([h, x])\n",
    "    x = Convolution2D(filters=channel_rate, kernel_size=(3, 3), padding='same')(x)\n",
    "    # PReLU can't be used, because it is connected with the input shape\n",
    "    # x = PReLU()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Output Image\n",
    "    outputs = Convolution2D(filters=3, kernel_size=(3, 3), padding='same', activation='tanh')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判别器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    # PatchGAN\n",
    "    inputs = Input(shape=patch_shape)\n",
    "    x = Convolution2D(filters=channel_rate, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Convolution2D(filters=2 * channel_rate, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Convolution2D(filters=4 * channel_rate, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Convolution2D(filters=4 * channel_rate, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(units=1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='PatchGAN')\n",
    "    # model.summary()\n",
    "    # discriminator\n",
    "    inputs = Input(shape=image_shape)\n",
    "    list_row_idx = [(i * channel_rate, (i + 1) * channel_rate) for i in\n",
    "                    range(int(image_shape[0] / patch_shape[0]))]\n",
    "    list_col_idx = [(i * channel_rate, (i + 1) * channel_rate) for i in\n",
    "                    range(int(image_shape[1] / patch_shape[1]))]\n",
    "    list_patch = []\n",
    "    for row_idx in list_row_idx:\n",
    "        for col_idx in list_col_idx:\n",
    "            x_patch = Lambda(lambda z: z[:, row_idx[0]:row_idx[1], col_idx[0]:col_idx[1], :])(inputs)\n",
    "            list_patch.append(x_patch)\n",
    "\n",
    "    x = [model(patch) for patch in list_patch]\n",
    "    outputs = Average()(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Discriminator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    inputs = Input(shape=image_shape)\n",
    "    generated_image = generator(inputs)\n",
    "    outputs = discriminator(generated_image)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# g = generator_model()\n",
    "# g.summary()\n",
    "# d = discriminator_model()\n",
    "# d.summary()\n",
    "# plot_model(d)\n",
    "# m = generator_containing_discriminator(generator_model(), discriminator_model())\n",
    "# m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "image_shape = (256, 256, 3)\n",
    "K_1 = 145\n",
    "K_2 = 170\n",
    "def l1_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    vgg = VGG16(include_top=False, weights='imagenet', input_shape=image_shape)\n",
    "    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
    "    # let the loss model can't be trained\n",
    "    loss_model.trainable = False\n",
    "    # loss_model.summary()\n",
    "    return K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))\n",
    "def generator_loss(y_true, y_pred):\n",
    "    return K_1 * perceptual_loss(y_true, y_pred) + K_2 * l1_loss(y_true, y_pred)\n",
    "def adversarial_loss(y_true, y_pred):\n",
    "    return -K.log(y_pred)\n",
    "#a, b = data_utils.format_image('data/small/test/301.jpg', size=256)\n",
    "#print(l1_loss(a.astype(np.float32), b.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练整个网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, epoch_num):\n",
    "    # Note the x(blur) in the second, the y(full) in the first\n",
    "    y_train, x_train = data_utils.load_data(data_type='train')\n",
    "    # GAN\n",
    "    g = generator_model()\n",
    "    d = discriminator_model()\n",
    "    d_on_g = generator_containing_discriminator(g, d)\n",
    "    # compile the models, use default optimizer parameters\n",
    "    # generator use adversarial loss\n",
    "    g.compile(optimizer='adam', loss=generator_loss)\n",
    "    # discriminator use binary cross entropy loss\n",
    "    d.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    # adversarial net use adversarial loss\n",
    "    d_on_g.compile(optimizer='adam', loss=adversarial_loss)\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        print('epoch: ', epoch + 1, '/', epoch_num)\n",
    "        print('batches: ', int(x_train.shape[0] / batch_size))\n",
    "        for index in range(int(x_train.shape[0] / batch_size)):\n",
    "            # select a batch data\n",
    "            image_blur_batch = x_train[index * batch_size:(index + 1) * batch_size]\n",
    "            image_full_batch = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "            generated_images = g.predict(x=image_blur_batch, batch_size=batch_size)\n",
    "\n",
    "            # output generated images for each 30 iters\n",
    "            if (index % 30 == 0) and (index != 0):\n",
    "                data_utils.generate_image(image_full_batch, image_blur_batch, generated_images,\n",
    "                                          'result/interim/', epoch, index)\n",
    "            # concatenate the full and generated images,\n",
    "            # the full images at top, the generated images at bottom\n",
    "            x = np.concatenate((image_full_batch, generated_images))\n",
    "\n",
    "            # generate labels for the full and generated images\n",
    "            y = [1] * batch_size + [0] * batch_size\n",
    "            # train discriminator\n",
    "            d_loss = d.train_on_batch(x, y)\n",
    "            print('batch %d d_loss : %f' % (index + 1, d_loss))\n",
    "            # let discriminator can't be trained\n",
    "            d.trainable = False\n",
    "            # train adversarial net\n",
    "            d_on_g_loss = d_on_g.train_on_batch(image_blur_batch, [1] * batch_size)\n",
    "            print('batch %d d_on_g_loss : %f' % (index + 1, d_on_g_loss))\n",
    "            # train generator\n",
    "            g_loss = g.train_on_batch(image_blur_batch, image_full_batch)\n",
    "            print('batch %d g_loss : %f' % (index + 1, g_loss))\n",
    "            # let discriminator can be trained\n",
    "            d.trainable = True\n",
    "            # output weights for generator and discriminator each 30 iters\n",
    "            if (index % 30 == 0) and (index != 0):\n",
    "                g.save_weights('weight/generator_weights.h5', True)\n",
    "                d.save_weights('weight/discriminator_weights.h5', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(batch_size):\n",
    "    # Note the x(blur) in the second, the y(full) in the first\n",
    "    y_test, x_test = data_utils.load_data(data_type='test')\n",
    "    g = generator_model()\n",
    "    g.load_weights('weight/generator_weights.h5')\n",
    "    generated_images = g.predict(x=x_test, batch_size=batch_size)\n",
    "    data_utils.generate_image(y_test, x_test, generated_images, 'result/finally/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pictures(batch_size):\n",
    "    data_path = 'data/test/*.jpeg'\n",
    "    images_path = gb.glob(data_path)\n",
    "    data_blur = []\n",
    "    for image_path in images_path:\n",
    "        image_blur = Image.open(image_path)\n",
    "        data_blur.append(np.array(image_blur))\n",
    "\n",
    "    data_blur = np.array(data_blur).astype(np.float32)\n",
    "    data_blur = data_utils.normalization(data_blur)\n",
    "\n",
    "    g = generator_model()\n",
    "    g.load_weights('weight/generator_weights.h5')\n",
    "    generated_images = g.predict(x=data_blur, batch_size=batch_size)\n",
    "    generated = generated_images * 127.5 + 127.5\n",
    "    for i in range(generated.shape[0]):\n",
    "        image_generated = generated[i, :, :, :]\n",
    "        Image.fromarray(image_generated.astype(np.uint8)).save('result/test/' + str(i) + '.png')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
