{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGANGP implement in keras\n",
    "* paper:[Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)\n",
    "* environment:keras==2.0.8 tensorflow==1.2\n",
    "* dataset:celeba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def mae(pred, target, name='mae'):\n",
    "    return tf.reduce_mean(tf.abs(pred - target), name=name)\n",
    "\n",
    "def mse(pred, target, name='mse'):\n",
    "    return tf.reduce_mean(tf.square(pred - target), name=name)\n",
    "\n",
    "def pixel_rmse(pred, target, name='rmse'):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(pred - target), name=name))\n",
    "\n",
    "def binary_cross_entropy_with_logits(pred, target):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=target))\n",
    "\n",
    "def wasserstein(pred, target):\n",
    "    return tf.reduce_mean(pred * target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import init\n",
    "from colorama import Fore, Back, Style\n",
    "import tensorflow as tf\n",
    "from terminaltables import SingleTable\n",
    "\n",
    "def print_table(TABLE_DATA):\n",
    "    table_instance = SingleTable(TABLE_DATA, \"\")\n",
    "    table_instance.justify_columns[2] = 'right'\n",
    "    print(table_instance.table)\n",
    "\n",
    "def print_bright(s):\n",
    "    init()\n",
    "    print(Style.BRIGHT + s + Style.RESET_ALL)\n",
    "\n",
    "def print_green(info, value=\"\"):\n",
    "    print(Fore.GREEN + \"[%s] \" % info + Style.RESET_ALL + str(value))\n",
    "\n",
    "def print_red(info, value=\"\"):\n",
    "    print(Fore.RED + \"[%s] \" % info + Style.RESET_ALL + str(value))\n",
    "\n",
    "def print_session():\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "    print_bright(\"\\nSetting up TF session:\")\n",
    "    for key in FLAGS.__dict__[\"__flags\"].keys():\n",
    "        if \"dir\" not in key:\n",
    "            print_green(key, FLAGS.__dict__[\"__flags\"][key])\n",
    "    print_bright(\"\\nConfiguring directories:\")\n",
    "    for d in [FLAGS.log_dir, FLAGS.model_dir, FLAGS.fig_dir]:\n",
    "        # Clear directories by default\n",
    "        if tf.gfile.Exists(d):\n",
    "            print_red(\"Deleting\", d)\n",
    "            tf.gfile.DeleteRecursively(d)\n",
    "    for d in [FLAGS.log_dir, FLAGS.model_dir, FLAGS.fig_dir]:\n",
    "        print_green(\"Creating\", d)\n",
    "        tf.gfile.MakeDirs(d)\n",
    "\n",
    "def print_initialize():\n",
    "    print_bright(\"\\nInitialization:\")\n",
    "    print_green(\"Created session saver\")\n",
    "    print_green(\"Ran init ops\")\n",
    "\n",
    "def print_summaries():\n",
    "    print_bright(\"\\nSummaries:\")\n",
    "    list_summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\n",
    "    for t in list_summaries:\n",
    "        print_green(t.name)\n",
    "\n",
    "def print_queues():\n",
    "    print_bright(\"\\nQueues:\")\n",
    "    print_green(\"Created coordinator\")\n",
    "    print_green(\"Started queue runner\")\n",
    "\n",
    "def print_check_data(out, list_data):\n",
    "    print\n",
    "    TABLE_DATA = (('Variable Name', 'Shape', \"Min value\", \"Max value\"),)\n",
    "    for o, t in zip(out, list_data):\n",
    "        TABLE_DATA += (tuple([t.name, str(o.shape), \"%.3g\" % o.min(), \"%.3g\" % o.max()]),)\n",
    "    print_table(TABLE_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "def save_image(data, data_format, e, suffix=None):\n",
    "    \"\"\"Saves a picture showing the current progress of the model\"\"\"\n",
    "\n",
    "    X_G, X_real = data\n",
    "\n",
    "    Xg = X_G[:8]\n",
    "    Xr = X_real[:8]\n",
    "\n",
    "    if data_format == \"NHWC\":\n",
    "        X = np.concatenate((Xg, Xr), axis=0)\n",
    "        list_rows = []\n",
    "        for i in range(int(X.shape[0] / 4)):\n",
    "            Xr = np.concatenate([X[k] for k in range(4 * i, 4 * (i + 1))], axis=1)\n",
    "            list_rows.append(Xr)\n",
    "\n",
    "        Xr = np.concatenate(list_rows, axis=0)\n",
    "\n",
    "    if data_format == \"NCHW\":\n",
    "        X = np.concatenate((Xg, Xr), axis=0)\n",
    "        list_rows = []\n",
    "        for i in range(int(X.shape[0] / 4)):\n",
    "            Xr = np.concatenate([X[k] for k in range(4 * i, 4 * (i + 1))], axis=2)\n",
    "            list_rows.append(Xr)\n",
    "\n",
    "        Xr = np.concatenate(list_rows, axis=1)\n",
    "        Xr = Xr.transpose(1,2,0)\n",
    "\n",
    "    if Xr.shape[-1] == 1:\n",
    "        plt.imshow(Xr[:, :, 0], cmap=\"gray\")\n",
    "    else:\n",
    "        plt.imshow(Xr)\n",
    "    plt.axis(\"off\")\n",
    "    if suffix is None:\n",
    "        plt.savefig(os.path.join(FLAGS.fig_dir, \"current_batch_%s.png\" % e))\n",
    "    else:\n",
    "        plt.savefig(os.path.join(FLAGS.fig_dir, \"current_batch_%s_%s.png\" % (suffix, e)))\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def get_stacked_tensor(X1, X2):\n",
    "\n",
    "    X = tf.concat((X1[:16], X2[:16]), axis=0)\n",
    "    list_rows = []\n",
    "    for i in range(8):\n",
    "        Xr = tf.concat([X[k] for k in range(4 * i, 4 * (i + 1))], axis=2)\n",
    "        list_rows.append(Xr)\n",
    "\n",
    "    X = tf.concat(list_rows, axis=1)\n",
    "    X = tf.transpose(X, (1,2,0))\n",
    "    X = tf.expand_dims(X, 0)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "import logging_utils as lu\n",
    "\n",
    "\n",
    "def setup_session():\n",
    "\n",
    "    lu.print_session()\n",
    "\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "    # Create session\n",
    "    config = tf.ConfigProto()\n",
    "    if FLAGS.use_XLA:\n",
    "        config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    # Setup directory to save model\n",
    "    for d in [FLAGS.log_dir, FLAGS.model_dir, FLAGS.fig_dir]:\n",
    "        # Clear directories by default\n",
    "        if tf.gfile.Exists(d):\n",
    "            tf.gfile.DeleteRecursively(d)\n",
    "        tf.gfile.MakeDirs(d)\n",
    "\n",
    "    # Initialize all RNGs with a deterministic seed\n",
    "    with sess.graph.as_default():\n",
    "        tf.set_random_seed(FLAGS.random_seed)\n",
    "\n",
    "    random.seed(FLAGS.random_seed)\n",
    "    np.random.seed(FLAGS.random_seed)\n",
    "\n",
    "    return sess\n",
    "\n",
    "\n",
    "def initialize_session(sess):\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "\n",
    "    lu.print_initialize()\n",
    "\n",
    "    return saver\n",
    "\n",
    "\n",
    "def add_gradient_summary(list_gradvar):\n",
    "    # Add summary for gradients\n",
    "    for g,v in list_gradvar:\n",
    "        if g is not None:\n",
    "            tf.summary.histogram(v.name + \"/gradient\", g)\n",
    "\n",
    "\n",
    "def manage_queues(sess):\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    lu.print_queues()\n",
    "\n",
    "    return coord\n",
    "\n",
    "\n",
    "def manage_summaries(sess):\n",
    "\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "    lu.print_summaries()\n",
    "\n",
    "    return writer\n",
    "\n",
    "\n",
    "def check_data(out, list_data):\n",
    "\n",
    "    lu.print_check_data(out, list_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    image = (image - 0.5) / 0.5\n",
    "    return image\n",
    "\n",
    "\n",
    "def unnormalize_image(image, name=None):\n",
    "\n",
    "    image = (image * 0.5 + 0.5) * 255.\n",
    "    image = tf.cast(image, tf.uint8, name=name)\n",
    "    return image\n",
    "\n",
    "\n",
    "def input_data(sess):\n",
    "\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "    list_images = glob.glob(os.path.join(FLAGS.celebA_path, \"*.jpg\"))\n",
    "\n",
    "    # Read each JPEG file\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        reader = tf.WholeFileReader()\n",
    "        filename_queue = tf.train.string_input_producer(list_images)\n",
    "        key, value = reader.read(filename_queue)\n",
    "        channels = FLAGS.channels\n",
    "        image = tf.image.decode_jpeg(value, channels=channels, name=\"dataset_image\")\n",
    "        image.set_shape([None, None, channels])\n",
    "\n",
    "        # Crop and other random augmentations\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        # image = tf.image.random_saturation(image, .95, 1.05)\n",
    "        # image = tf.image.random_brightness(image, .05)\n",
    "        # image = tf.image.random_contrast(image, .95, 1.05)\n",
    "\n",
    "        # Center crop\n",
    "        image = tf.image.central_crop(image, FLAGS.central_fraction)\n",
    "\n",
    "        # Resize\n",
    "        image = tf.image.resize_images(image, (FLAGS.img_size, FLAGS.img_size), method=tf.image.ResizeMethod.AREA)\n",
    "\n",
    "        # Normalize\n",
    "        image = normalize_image(image)\n",
    "\n",
    "        # Format image to correct ordering\n",
    "        if FLAGS.data_format == \"NCHW\":\n",
    "            image = tf.transpose(image, (2,0,1))\n",
    "\n",
    "        # Using asynchronous queues\n",
    "        img_batch = tf.train.batch([image],\n",
    "                                   batch_size=FLAGS.batch_size,\n",
    "                                   num_threads=FLAGS.num_threads,\n",
    "                                   capacity=2 * FLAGS.num_threads * FLAGS.batch_size,\n",
    "                                   name='X_real_input')\n",
    "\n",
    "        return img_batch\n",
    "\n",
    "\n",
    "def sample_batch(X, batch_size):\n",
    "\n",
    "    idx = np.random.choice(X.shape[0], batch_size, replace=False)\n",
    "    return X[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import layers\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def get_trainable_variables(self):\n",
    "        t_vars = tf.trainable_variables()\n",
    "        t_vars_model = {v.name: v for v in t_vars if self.name in v.name}\n",
    "        return t_vars_model\n",
    "\n",
    "\n",
    "class Generator(Model):\n",
    "    def __init__(self, list_filters, list_kernel_size, list_strides, list_padding, output_shape,\n",
    "                 name=\"generator\", batch_size=32, filters=512, dset=\"celebA\", data_format=\"NCHW\"):\n",
    "\n",
    "        super(Generator, self).__init__(name)\n",
    "\n",
    "        self.data_format = data_format\n",
    "\n",
    "        if self.data_format == \"NCHW\":\n",
    "            self.output_h = output_shape[1]\n",
    "            self.output_w = output_shape[2]\n",
    "        else:\n",
    "            self.output_h = output_shape[0]\n",
    "            self.output_w = output_shape[1]\n",
    "\n",
    "        if dset == \"mnist\":\n",
    "            self.start_dim = int(self.output_h / 4)\n",
    "            self.nb_upconv = 2\n",
    "        else:\n",
    "            self.start_dim = int(self.output_h / 16)\n",
    "            self.nb_upconv = 4\n",
    "\n",
    "        self.output_shape = output_shape\n",
    "        self.dset = dset\n",
    "        self.name = name\n",
    "        self.batch_size = batch_size\n",
    "        self.filters = filters\n",
    "        self.list_filters = list_filters\n",
    "        self.list_kernel_size = list_kernel_size\n",
    "        self.list_padding = list_padding\n",
    "        self.list_strides = list_strides\n",
    "\n",
    "    def __call__(self, x, reuse=False):\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "\n",
    "            if reuse:\n",
    "                # list_v = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)\n",
    "                # for v in list_v:\n",
    "                #     print v\n",
    "                # print\n",
    "                # print\n",
    "                # for v in tf.get_collection(tf.GraphKeys.UPDATE_OPS):\n",
    "                #     print v\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            # Store all layers in a dict\n",
    "            d = collections.OrderedDict()\n",
    "\n",
    "            # Initial dense multiplication\n",
    "            x = layers.linear(x, self.filters * self.start_dim * self.start_dim)\n",
    "\n",
    "            # Reshape to image format\n",
    "            if self.data_format == \"NCHW\":\n",
    "                target_shape = (self.batch_size, self.filters, self.start_dim, self.start_dim)\n",
    "            else:\n",
    "                target_shape = (self.batch_size, self.start_dim, self.start_dim, self.filters)\n",
    "\n",
    "            x = layers.reshape(x, target_shape)\n",
    "            x = tf.contrib.layers.batch_norm(x, fused=True)\n",
    "            x = tf.nn.relu(x)\n",
    "\n",
    "            # # Conv2D + Phase shift blocks\n",
    "            # x = layers.conv2d_block(\"conv2D_1_1\", x, 512, 3, 1, p=\"SAME\", stddev=0.02,\n",
    "            #                         data_format=self.data_format, bias=False, bn=True, activation_fn=layers.lrelu)\n",
    "            # x = layers.conv2d_block(\"conv2D_1_2\", x, 512, 3, 1, p=\"SAME\", stddev=0.02,\n",
    "            #                         data_format=self.data_format, bias=False, bn=False, activation_fn=layers.lrelu)\n",
    "            # x = layers.phase_shift(x, upsampling_factor=2, data_format=self.data_format, name=\"PS1\")\n",
    "\n",
    "            # x = layers.conv2d_block(\"conv2D_2_1\", x, 256, 3, 1, p=\"SAME\", stddev=0.02,\n",
    "            #                         data_format=self.data_format, bias=False, bn=False, activation_fn=layers.lrelu)\n",
    "            # x = layers.conv2d_block(\"conv2D_2_2\", x, 256, 3, 1, p=\"SAME\", stddev=0.02,\n",
    "            #                         data_format=self.data_format, bias=False, bn=False, activation_fn=layers.lrelu)\n",
    "            # x = layers.phase_shift(x, upsampling_factor=2, data_format=self.data_format, name=\"PS2\")\n",
    "\n",
    "            # x = layers.conv2d_block(\"conv2D_3\", x, 1, 1, 1, p=\"SAME\", stddev=0.02,\n",
    "            #                         data_format=self.data_format, bn=False)\n",
    "\n",
    "            # # Upsampling2D + conv blocks\n",
    "            for idx, (f, k, s, p) in enumerate(zip(self.list_filters, self.list_kernel_size, self.list_strides, self.list_padding)):\n",
    "                name = \"upsample2D_%s\" % idx\n",
    "                if idx == len(self.list_filters) - 1:\n",
    "                    bn = False\n",
    "                    activation_fn = None\n",
    "                else:\n",
    "                    bn = True\n",
    "                    activation_fn = tf.nn.relu\n",
    "                x = layers.upsample2d_block(name, x, f, k, s, p, data_format=self.data_format, bn=bn, activation_fn=activation_fn)\n",
    "\n",
    "            # # Transposed conv blocks\n",
    "            # for idx, (f, k, s, p) in enumerate(zip(self.list_filters, self.list_kernel_size, self.list_strides, self.list_padding)):\n",
    "            #     img_size = self.start_dim * (2 ** (idx + 1))\n",
    "            #     if self.data_format == \"NCHW\":\n",
    "            #         output_shape = (self.batch_size, f, img_size, img_size)\n",
    "            #     else:\n",
    "            #         output_shape = (self.batch_size, img_size, img_size, f)\n",
    "            #     name = \"deconv2D_%s\" % idx\n",
    "            #     if idx == len(self.list_filters) - 1:\n",
    "            #         bn = False\n",
    "            #         activation_fn = None\n",
    "            #     else:\n",
    "            #         bn = True\n",
    "            #         activation_fn = layers.lrelu\n",
    "            #     x = layers.deconv2d_block(name, x, output_shape, k, s, p, data_format=self.data_format, bn=bn, activation_fn=activation_fn)\n",
    "\n",
    "            x = tf.nn.tanh(x, name=\"X_G\")\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "class Discriminator(Model):\n",
    "    def __init__(self, list_filters, list_kernel_size, list_strides, list_padding, batch_size,\n",
    "                 name=\"discriminator\", data_format=\"NCHW\"):\n",
    "        # Determine data format from output shape\n",
    "\n",
    "        super(Discriminator, self).__init__(name)\n",
    "\n",
    "        self.data_format = data_format\n",
    "        self.name = name\n",
    "        self.list_filters = list_filters\n",
    "        self.list_strides = list_strides\n",
    "        self.list_kernel_size = list_kernel_size\n",
    "        self.batch_size = batch_size\n",
    "        self.list_padding = list_padding\n",
    "\n",
    "    def __call__(self, x, reuse=False):\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            for idx, (f, k, s, p) in enumerate(zip(self.list_filters, self.list_kernel_size, self.list_strides, self.list_padding)):\n",
    "                if idx == 0:\n",
    "                    bn = False\n",
    "                else:\n",
    "                    bn = True\n",
    "                name = \"conv2D_%s\" % idx\n",
    "                x = layers.conv2d_block(name, x, f, k, s, p=p, stddev=0.02,\n",
    "                                        data_format=self.data_format, bias=True, bn=bn, activation_fn=layers.lrelu)\n",
    "\n",
    "            target_shape = (self.batch_size, -1)\n",
    "            x = layers.reshape(x, target_shape)\n",
    "\n",
    "            # # Add MBD\n",
    "            # x_mbd = layers.mini_batch_disc(x, num_kernels=100, dim_per_kernel=5)\n",
    "            # # Concat\n",
    "            # x = tf.concat([x, x_mbd], axis=1)\n",
    "\n",
    "            x = layers.linear(x, 1, bias=False)\n",
    "\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import models\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "def train_model():\n",
    "    # Setup session\n",
    "    sess = setup_session()\n",
    "    # Setup async input queue of real images\n",
    "    X_real = input_data(sess)\n",
    "    #######################\n",
    "    # Instantiate generator\n",
    "    #######################\n",
    "    list_filters = [256, 128, 64, 3]\n",
    "    list_strides = [2] * len(list_filters)\n",
    "    list_kernel_size = [3] * len(list_filters)\n",
    "    list_padding = [\"SAME\"] * len(list_filters)\n",
    "    output_shape = X_real.get_shape().as_list()[1:]\n",
    "    G = models.Generator(list_filters, list_kernel_size, list_strides, list_padding, output_shape,\n",
    "                         batch_size=FLAGS.batch_size, data_format=FLAGS.data_format)\n",
    "\n",
    "    ###########################\n",
    "    # Instantiate discriminator\n",
    "    ###########################\n",
    "    list_filters = [32, 64, 128, 256]\n",
    "    list_strides = [2] * len(list_filters)\n",
    "    list_kernel_size = [3] * len(list_filters)\n",
    "    list_padding = [\"SAME\"] * len(list_filters)\n",
    "    D = models.Discriminator(list_filters, list_kernel_size, list_strides, list_padding,\n",
    "                             FLAGS.batch_size, data_format=FLAGS.data_format)\n",
    "    ###########################\n",
    "    # Instantiate optimizers\n",
    "    ###########################\n",
    "    G_opt = tf.train.AdamOptimizer(learning_rate=1E-4, name='G_opt', beta1=0.5, beta2=0.9)\n",
    "    D_opt = tf.train.AdamOptimizer(learning_rate=1E-4, name='D_opt', beta1=0.5, beta2=0.9)\n",
    "    ###########################\n",
    "    # Instantiate model outputs\n",
    "    ###########################\n",
    "    # noise_input = tf.random_normal((FLAGS.batch_size, FLAGS.noise_dim,), stddev=0.1)\n",
    "    noise_input = tf.random_uniform((FLAGS.batch_size, FLAGS.noise_dim,), minval=-1, maxval=1)\n",
    "    X_fake = G(noise_input)\n",
    "    # output images\n",
    "    X_G_output = du.unnormalize_image(X_fake)\n",
    "    X_real_output = du.unnormalize_image(X_real)\n",
    "\n",
    "    D_real = D(X_real)\n",
    "    D_fake = D(X_fake, reuse=True)\n",
    "\n",
    "    ###########################\n",
    "    # Instantiate losses\n",
    "    ###########################\n",
    "    G_loss = -tf.reduce_mean(D_fake)\n",
    "    D_loss = tf.reduce_mean(D_fake) - tf.reduce_mean(D_real)\n",
    "    epsilon = tf.random_uniform(\n",
    "        shape=[FLAGS.batch_size, 1, 1, 1],\n",
    "        minval=0.,\n",
    "        maxval=1.\n",
    "    )\n",
    "    X_hat = X_real + epsilon * (X_fake - X_real)\n",
    "    D_X_hat = D(X_hat, reuse=True)\n",
    "    grad_D_X_hat = tf.gradients(D_X_hat, [X_hat])[0]\n",
    "    if FLAGS.data_format == \"NCHW\":\n",
    "        red_idx = [1]\n",
    "    else:\n",
    "        red_idx = [-1]\n",
    "    slopes = tf.sqrt(tf.reduce_sum(tf.square(grad_D_X_hat), reduction_indices=red_idx))\n",
    "    gradient_penalty = tf.reduce_mean((slopes - 1.)**2)\n",
    "    D_loss += 10 * gradient_penalty\n",
    "\n",
    "    ###########################\n",
    "    # Compute gradient updates\n",
    "    ###########################\n",
    "\n",
    "    dict_G_vars = G.get_trainable_variables()\n",
    "    G_vars = [dict_G_vars[k] for k in dict_G_vars.keys()]\n",
    "\n",
    "    dict_D_vars = D.get_trainable_variables()\n",
    "    D_vars = [dict_D_vars[k] for k in dict_D_vars.keys()]\n",
    "\n",
    "    G_gradvar = G_opt.compute_gradients(G_loss, var_list=G_vars, colocate_gradients_with_ops=True)\n",
    "    G_update = G_opt.apply_gradients(G_gradvar, name='G_loss_minimize')\n",
    "\n",
    "    D_gradvar = D_opt.compute_gradients(D_loss, var_list=D_vars, colocate_gradients_with_ops=True)\n",
    "    D_update = D_opt.apply_gradients(D_gradvar, name='D_loss_minimize')\n",
    "\n",
    "    ##########################\n",
    "    # Group training ops\n",
    "    ##########################\n",
    "    loss_ops = [G_loss, D_loss]\n",
    "    ##########################\n",
    "    # Summary ops\n",
    "    ##########################\n",
    "    # Add summary for gradients\n",
    "    tu.add_gradient_summary(G_gradvar)\n",
    "    tu.add_gradient_summary(D_gradvar)\n",
    "\n",
    "    # Add scalar symmaries\n",
    "    tf.summary.scalar(\"G loss\", G_loss)\n",
    "    tf.summary.scalar(\"D loss\", D_loss)\n",
    "    tf.summary.scalar(\"gradient_penalty\", gradient_penalty)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    ############################\n",
    "    # Start training\n",
    "    ############################\n",
    "\n",
    "    # Initialize session\n",
    "    saver = initialize_session(sess)\n",
    "    # Start queues\n",
    "    manage_queues(sess)\n",
    "\n",
    "    # Summaries\n",
    "    writer = manage_summaries(sess)\n",
    "\n",
    "    for e in tqdm(range(FLAGS.nb_epoch), desc=\"Training progress\"):\n",
    "\n",
    "        t = tqdm(range(FLAGS.nb_batch_per_epoch), desc=\"Epoch %i\" % e, mininterval=0.5)\n",
    "        for batch_counter in t:\n",
    "\n",
    "            for di in range(5):\n",
    "                sess.run([D_update])\n",
    "\n",
    "            output = sess.run([G_update] + loss_ops + [summary_op])\n",
    "\n",
    "            if batch_counter % (FLAGS.nb_batch_per_epoch // 20) == 0:\n",
    "                writer.add_summary(output[-1], e * FLAGS.nb_batch_per_epoch + batch_counter)\n",
    "\n",
    "            t.set_description('Epoch %i' % e)\n",
    "\n",
    "        # Plot some generated images\n",
    "        output = sess.run([X_G_output, X_real_output])\n",
    "        vu.save_image(output, FLAGS.data_format, e)\n",
    "\n",
    "        # Save session\n",
    "        saver.save(sess, os.path.join(FLAGS.model_dir, \"model\"), global_step=e)\n",
    "\n",
    "    print('Finished training!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### paprameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "def define_flags():\n",
    "\n",
    "    ############\n",
    "    # Run mode\n",
    "    ############\n",
    "    tf.app.flags.DEFINE_string('run', None, \"Which operation to run. [train|inference]\")\n",
    "\n",
    "    ##########################\n",
    "    # Training parameters\n",
    "    ###########################\n",
    "    tf.app.flags.DEFINE_integer('nb_epoch', 400, \"Number of epochs\")\n",
    "    tf.app.flags.DEFINE_integer('batch_size', 256, \"Number of samples per batch.\")\n",
    "    tf.app.flags.DEFINE_integer('nb_batch_per_epoch', 50, \"Number of batches per epoch\")\n",
    "    tf.app.flags.DEFINE_float('learning_rate', 2E-4, \"Learning rate used for AdamOptimizer\")\n",
    "    tf.app.flags.DEFINE_integer('noise_dim', 100, \"Noise dimension for GAN generation\")\n",
    "    tf.app.flags.DEFINE_integer('random_seed', 0, \"Seed used to initialize rng.\")\n",
    "\n",
    "    ############################################\n",
    "    # General tensorflow parameters parameters\n",
    "    #############################################\n",
    "    tf.app.flags.DEFINE_bool('use_XLA', False, \"Whether to use XLA compiler.\")\n",
    "    tf.app.flags.DEFINE_integer('num_threads', 2, \"Number of threads to fetch the data\")\n",
    "    tf.app.flags.DEFINE_float('capacity_factor', 32, \"Nuumber of batches to store in queue\")\n",
    "\n",
    "    ##########\n",
    "    # Datasets\n",
    "    ##########\n",
    "    tf.app.flags.DEFINE_string('data_format', \"NHWC\", \"Tensorflow image data format.\")\n",
    "    tf.app.flags.DEFINE_string('celebA_path', \"../../data/raw/img_align_celeba\", \"Path to celebA images\")\n",
    "    tf.app.flags.DEFINE_integer('channels', 3, \"Number of channels\")\n",
    "    tf.app.flags.DEFINE_float('central_fraction', 0.8, \"Central crop as a fraction of total image\")\n",
    "    tf.app.flags.DEFINE_integer('img_size', 64, \"Image size\")\n",
    "\n",
    "    ##############\n",
    "    # Directories\n",
    "    ##############\n",
    "    tf.app.flags.DEFINE_string('model_dir', '../../models', \"Output folder where checkpoints are dumped.\")\n",
    "    tf.app.flags.DEFINE_string('log_dir', '../../logs', \"Logs for tensorboard.\")\n",
    "    tf.app.flags.DEFINE_string('fig_dir', '../../figures', \"Where to save figures.\")\n",
    "    tf.app.flags.DEFINE_string('raw_dir', '../../data/raw', \"Where raw data is saved\")\n",
    "    tf.app.flags.DEFINE_string('data_dir', '../../data/processed', \"Where processed data is saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable Tensorflow's INFO and WARNING messages\n",
    "# See http://stackoverflow.com/questions/35911252\n",
    "if 'TF_CPP_MIN_LOG_LEVEL' not in os.environ:\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import flags\n",
    "import tensorflow as tf\n",
    "import train_wgan_GP\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "def launch_training():\n",
    "\n",
    "    train_wgan_GP.train_model()\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "\n",
    "    assert FLAGS.run in [\"train\", \"inference\"], \"Choose [train|inference]\"\n",
    "\n",
    "    if FLAGS.run == 'train':\n",
    "        launch_training()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.define_flags()\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
