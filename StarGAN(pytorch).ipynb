{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T10:47:01.884719Z",
     "start_time": "2018-03-20T10:47:01.344719Z"
    },
    "lang": "en"
   },
   "source": [
    "# SSGAN pytorch\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "PyTorch implementation of [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https://arxiv.org/abs/1711.09020). StarGAN can flexibly translate an input image to any desired target domain using only a single generator and a discriminator. The demo video for StarGAN can be found [here](https://www.youtube.com/watch?v=EYjdLppmERE).\n",
    "\n",
    "\n",
    "## Authors\n",
    "\n",
    "[Yunjey Choi](https://github.com/yunjey), [Minje Choi](https://github.com/mjc92), [Munyoung Kim](https://www.facebook.com/munyoung.kim.1291), [Jung-Woo Ha](https://www.facebook.com/jungwoo.ha.921), [Sung Kim](https://www.cse.ust.hk/~hunkim/), and [Jaegul Choo](https://sites.google.com/site/jaegulchoo/)    \n",
    "Korea Universitiy, Clova AI Research (NAVER), The College of New Jersey, HKUST  \n",
    "&nbsp;\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "#### Facial Attribute Transfer on CelebA\n",
    "The images are generated by StarGAN trained on the CelebA dataset.\n",
    "\n",
    "\n",
    "#### Facial Expression Synthesis on RaFD\n",
    "The images are generated by StarGAN trained on the RaFD dataset.\n",
    "\n",
    "\n",
    "#### Facial Expression Synthesis on CelebA\n",
    "The images are generated by StarGAN trained on both the CelebA and RaFD dataset.\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## Model Description\n",
    "### Training within a Single Dataset\n",
    "Overview of StarGAN, consisting of two modules, a discriminator <b>D</b> and a generator <b>G</b>. <b>(a)</b> <b>D</b> learns to distinguish between real and fake images and classify the real images to its corresponding domain. <b>(b)</b> <b>G</b> takes in as input both the image and target domain label and generates an fake image. The target domain label is spatially replicated and concatenated with the input image. <b>(c)</b> <b>G</b> tries to reconstruct the original image from the fake image given the original domain label. <b>(d)</b> <b>G</b> tries to generate images indistinguishable from real images and classifiable as target domain by <b>D</b>.\n",
    "\n",
    "\n",
    "### Training with Multiple Datasets\n",
    "Overview of StarGAN when training with both CelebA and RaFD. <b>(a) ~ (d)</b> shows the training process using CelebA, and <b>(e) ~ (h)</b> shows the training process using RaFD. <b>(a), (e)</b> The discriminator <b>D</b> learns to distinguish between real and fake images and minimize the classification error only for the known label. <b>(b), (c), (f), (g)</b> When the mask vector (purple) is [1, 0], the generator <b>G</b> learns to focus on the CelebA label (yellow) and ignore the RaFD label (green) to perform image-to-image translation, and vice versa when the mask vector is [0, 1]. <b>(d), (h)</b> <b>G</b> tries to generate images that are both indistinguishable from real images and classifiable by <b>D</b> as belonging to the target domain.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## Prerequisites\n",
    "* [Python 3.5+](https://www.continuum.io/downloads)\n",
    "* [PyTorch 0.2.0](http://pytorch.org/)\n",
    "* [TensorFlow 1.3+](https://www.tensorflow.org/) (optional for tensorboard)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "#### 1. Clone the repository\n",
    "```bash\n",
    "$ git clone https://github.com/yunjey/StarGAN.git\n",
    "$ cd StarGAN/\n",
    "```\n",
    "\n",
    "#### 2. Download the dataset\n",
    "##### (i) CelebA dataset\n",
    "```bash\n",
    "$ bash download.sh\n",
    "```\n",
    "\n",
    "##### (ii) RaFD dataset\n",
    "Because <b>RaFD</b> is not a public dataset, you must first request access to the dataset from [the Radboud Faces Database website](http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main). Then, you need to create the folder structure as decribed [here.](https://github.com/yunjey/StarGAN/blob/master/png/RaFD.md)\n",
    "\n",
    "#### 3. Train StarGAN\n",
    "##### (i) Training with CelebA\n",
    "\n",
    "```bash\n",
    "$ python main.py --mode='train' --dataset='CelebA' --c_dim=5 --image_size=128 \\\n",
    "                 --sample_path='stargan_celebA/samples' --log_path='stargan_celebA/logs' \\\n",
    "                 --model_save_path='stargan_celebA/models' --result_path='stargan_celebA/results'\n",
    "```\n",
    "##### (ii) Training with RaFD\n",
    "\n",
    "```bash\n",
    "$ python main.py --mode='train' --dataset='RaFD' --c_dim=8 --image_size=128 \\\n",
    "                 --num_epochs=200 --num_epochs_decay=100 --sample_step=200 --model_save_step=200 \\\n",
    "                 --sample_path='stargan_rafd/samples' --log_path='stargan_rafd/logs' \\\n",
    "                 --model_save_path='stargan_rafd/models' --result_path='stargan_rafd/results'\n",
    "```\n",
    "\n",
    "##### (iii) Training with CelebA+RaFD\n",
    "\n",
    "```bash\n",
    "$ python main.py --mode='train' --dataset='Both' --image_size=256 --num_iters=200000 --num_iters_decay=100000 \\\n",
    "                 --sample_path='stargan_both/samples' --log_path='stargan_both/logs' \\\n",
    "                 --model_save_path='stargan_both/models' --result_path='stargan_both/results'\n",
    "```\n",
    "\n",
    "#### 4. Test StarGAN\n",
    "##### (i) Facial attribute transfer on CelebA\n",
    "```bash\n",
    "$ python main.py --mode='test' --dataset='CelebA' --c_dim=5 --image_size=128 --test_model='20_1000' \\\n",
    "                 --sample_path='stargan_celebA/samples' --log_path='stargan_celebA/logs' \\\n",
    "                 --model_save_path='stargan_celebA/models' --result_path='stargan_celebA/results'\n",
    "```\n",
    "\n",
    "##### (ii) Facial expression synthesis on RaFD\n",
    "```bash\n",
    "$ python main.py --mode='test' --dataset='RaFD' --c_dim=8 --image_size=128 \\\n",
    "                 --test_model='200_200' --rafd_image_path='data/RaFD/test' \\\n",
    "                 --sample_path='stargan_rafd/samples' --log_path='stargan_rafd/logs' \\\n",
    "                 --model_save_path='stargan_rafd/models' --result_path='stargan_rafd/results'\n",
    "```\n",
    "\n",
    "##### (iii) Facial expression synthesis on CelebA\n",
    "```bash\n",
    "$ python main.py --mode='test' --dataset='Both' --image_size=256 --test_model='200000' \\\n",
    "                 --sample_path='stargan_both/samples' --log_path='stargan_both/logs' \\\n",
    "                 --model_save_path='stargan_both/models' --result_path='stargan_both/results'\n",
    "```\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## Citation\n",
    "If ts useful for your research, please cite our [arXiv paper](https://arxiv.org/abs/1711.09020).\n",
    "```\n",
    "@article{choi2017stargan,\n",
    " title = {StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},    \n",
    " author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},\n",
    " journal= {arXiv preprint arXiv:1711.09020},\n",
    " Year = {2017}\n",
    "}\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "## Acknowledgement\n",
    "This work was mainly done while the first author did a research internship at <b>Clova AI Research, NAVER (CLAIR)</b>. We also thank all the researchers at CLAIR, especially Donghyun Kwak, for insightful discussions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.backends import cudnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc \n",
    "try:\n",
    "    from StringIO import StringIO  # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO         # Python 3.5+\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "\n",
    "class CelebDataset(Dataset):\n",
    "    def __init__(self, image_path, metadata_path, transform, mode):\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.lines = open(metadata_path, 'r').readlines()\n",
    "        self.num_data = int(self.lines[0])\n",
    "        self.attr2idx = {}\n",
    "        self.idx2attr = {}\n",
    "\n",
    "        print ('Start preprocessing dataset..!')\n",
    "        random.seed(1234)\n",
    "        self.preprocess()\n",
    "        print ('Finished preprocessing dataset..!')\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.num_data = len(self.train_filenames)\n",
    "        elif self.mode == 'test':\n",
    "            self.num_data = len(self.test_filenames)\n",
    "\n",
    "    def preprocess(self):\n",
    "        attrs = self.lines[1].split()\n",
    "        for i, attr in enumerate(attrs):\n",
    "            self.attr2idx[attr] = i\n",
    "            self.idx2attr[i] = attr\n",
    "\n",
    "        self.selected_attrs = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young']\n",
    "        self.train_filenames = []\n",
    "        self.train_labels = []\n",
    "        self.test_filenames = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        lines = self.lines[2:]\n",
    "        random.shuffle(lines)   # random shuffling\n",
    "        for i, line in enumerate(lines):\n",
    "\n",
    "            splits = line.split()\n",
    "            filename = splits[0]\n",
    "            values = splits[1:]\n",
    "\n",
    "            label = []\n",
    "            for idx, value in enumerate(values):\n",
    "                attr = self.idx2attr[idx]\n",
    "\n",
    "                if attr in self.selected_attrs:\n",
    "                    if value == '1':\n",
    "                        label.append(1)\n",
    "                    else:\n",
    "                        label.append(0)\n",
    "\n",
    "            if (i+1) < 2000:\n",
    "                self.test_filenames.append(filename)\n",
    "                self.test_labels.append(label)\n",
    "            else:\n",
    "                self.train_filenames.append(filename)\n",
    "                self.train_labels.append(label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train':\n",
    "            image = Image.open(os.path.join(self.image_path, self.train_filenames[index]))\n",
    "            label = self.train_labels[index]\n",
    "        elif self.mode in ['test']:\n",
    "            image = Image.open(os.path.join(self.image_path, self.test_filenames[index]))\n",
    "            label = self.test_labels[index]\n",
    "\n",
    "        return self.transform(image), torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "\n",
    "def get_loader(image_path, metadata_path, crop_size, image_size, batch_size, dataset='CelebA', mode='train'):\n",
    "    \"\"\"Build and return data loader.\"\"\"\n",
    "\n",
    "    if mode == 'train':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.Resize(image_size, interpolation=Image.ANTIALIAS),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.Scale(image_size, interpolation=Image.ANTIALIAS),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    if dataset == 'CelebA':\n",
    "        dataset = CelebDataset(image_path, metadata_path, transform, mode)\n",
    "    elif dataset == 'RaFD':\n",
    "        dataset = ImageFolder(image_path, transform)\n",
    "\n",
    "    shuffle = False\n",
    "    if mode == 'train':\n",
    "        shuffle = True\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "from model import Generator\n",
    "from model import Discriminator\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Solver(object):\n",
    "\n",
    "    def __init__(self, celebA_loader, rafd_loader, config):\n",
    "        # Data loader\n",
    "        self.celebA_loader = celebA_loader\n",
    "        self.rafd_loader = rafd_loader\n",
    "\n",
    "        # Model hyper-parameters\n",
    "        self.c_dim = config.c_dim\n",
    "        self.c2_dim = config.c2_dim\n",
    "        self.image_size = config.image_size\n",
    "        self.g_conv_dim = config.g_conv_dim\n",
    "        self.d_conv_dim = config.d_conv_dim\n",
    "        self.g_repeat_num = config.g_repeat_num\n",
    "        self.d_repeat_num = config.d_repeat_num\n",
    "        self.d_train_repeat = config.d_train_repeat\n",
    "\n",
    "        # Hyper-parameteres\n",
    "        self.lambda_cls = config.lambda_cls\n",
    "        self.lambda_rec = config.lambda_rec\n",
    "        self.lambda_gp = config.lambda_gp\n",
    "        self.g_lr = config.g_lr\n",
    "        self.d_lr = config.d_lr\n",
    "        self.beta1 = config.beta1\n",
    "        self.beta2 = config.beta2\n",
    "\n",
    "        # Training settings\n",
    "        self.dataset = config.dataset\n",
    "        self.num_epochs = config.num_epochs\n",
    "        self.num_epochs_decay = config.num_epochs_decay\n",
    "        self.num_iters = config.num_iters\n",
    "        self.num_iters_decay = config.num_iters_decay\n",
    "        self.batch_size = config.batch_size\n",
    "        self.use_tensorboard = config.use_tensorboard\n",
    "        self.pretrained_model = config.pretrained_model\n",
    "\n",
    "        # Test settings\n",
    "        self.test_model = config.test_model\n",
    "\n",
    "        # Path\n",
    "        self.log_path = config.log_path\n",
    "        self.sample_path = config.sample_path\n",
    "        self.model_save_path = config.model_save_path\n",
    "        self.result_path = config.result_path\n",
    "\n",
    "        # Step size\n",
    "        self.log_step = config.log_step\n",
    "        self.sample_step = config.sample_step\n",
    "        self.model_save_step = config.model_save_step\n",
    "\n",
    "        # Build tensorboard if use\n",
    "        self.build_model()\n",
    "        if self.use_tensorboard:\n",
    "            self.build_tensorboard()\n",
    "\n",
    "        # Start with trained model\n",
    "        if self.pretrained_model:\n",
    "            self.load_pretrained_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Define a generator and a discriminator\n",
    "        if self.dataset == 'Both':\n",
    "            self.G = Generator(self.g_conv_dim, self.c_dim+self.c2_dim+2, self.g_repeat_num)   # 2 for mask vector\n",
    "            self.D = Discriminator(self.image_size, self.d_conv_dim, self.c_dim+self.c2_dim, self.d_repeat_num)\n",
    "        else:\n",
    "            self.G = Generator(self.g_conv_dim, self.c_dim, self.g_repeat_num)\n",
    "            self.D = Discriminator(self.image_size, self.d_conv_dim, self.c_dim, self.d_repeat_num) \n",
    "\n",
    "        # Optimizers\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
    "\n",
    "        # Print networks\n",
    "        self.print_network(self.G, 'G')\n",
    "        self.print_network(self.D, 'D')\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "\n",
    "    def print_network(self, model, name):\n",
    "        num_params = 0\n",
    "        for p in model.parameters():\n",
    "            num_params += p.numel()\n",
    "        print(name)\n",
    "        print(model)\n",
    "        print(\"The number of parameters: {}\".format(num_params))\n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        self.G.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_G.pth'.format(self.pretrained_model))))\n",
    "        self.D.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_D.pth'.format(self.pretrained_model))))\n",
    "        print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n",
    "\n",
    "    def build_tensorboard(self):\n",
    "        from logger import Logger\n",
    "        self.logger = Logger(self.log_path)\n",
    "\n",
    "    def update_lr(self, g_lr, d_lr):\n",
    "        for param_group in self.g_optimizer.param_groups:\n",
    "            param_group['lr'] = g_lr\n",
    "        for param_group in self.d_optimizer.param_groups:\n",
    "            param_group['lr'] = d_lr\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "    def to_var(self, x, volatile=False):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return Variable(x, volatile=volatile)\n",
    "\n",
    "    def denorm(self, x):\n",
    "        out = (x + 1) / 2\n",
    "        return out.clamp_(0, 1)\n",
    "\n",
    "    def threshold(self, x):\n",
    "        x = x.clone()\n",
    "        x = (x >= 0.5).float()\n",
    "        return x\n",
    "\n",
    "    def compute_accuracy(self, x, y, dataset):\n",
    "        if dataset == 'CelebA':\n",
    "            x = F.sigmoid(x)\n",
    "            predicted = self.threshold(x)\n",
    "            correct = (predicted == y).float()\n",
    "            accuracy = torch.mean(correct, dim=0) * 100.0\n",
    "        else:\n",
    "            _, predicted = torch.max(x, dim=1)\n",
    "            correct = (predicted == y).float()\n",
    "            accuracy = torch.mean(correct) * 100.0\n",
    "        return accuracy\n",
    "\n",
    "    def one_hot(self, labels, dim):\n",
    "        \"\"\"Convert label indices to one-hot vector\"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        out = torch.zeros(batch_size, dim)\n",
    "        out[np.arange(batch_size), labels.long()] = 1\n",
    "        return out\n",
    "\n",
    "    def make_celeb_labels(self, real_c):\n",
    "        \"\"\"Generate domain labels for CelebA for debugging/testing.\n",
    "\n",
    "        if dataset == 'CelebA':\n",
    "            return single and multiple attribute changes\n",
    "        elif dataset == 'Both':\n",
    "            return single attribute changes\n",
    "        \"\"\"\n",
    "        y = [torch.FloatTensor([1, 0, 0]),  # black hair\n",
    "             torch.FloatTensor([0, 1, 0]),  # blond hair\n",
    "             torch.FloatTensor([0, 0, 1])]  # brown hair\n",
    "\n",
    "        fixed_c_list = []\n",
    "\n",
    "        # single attribute transfer\n",
    "        for i in range(self.c_dim):\n",
    "            fixed_c = real_c.clone()\n",
    "            for c in fixed_c:\n",
    "                if i < 3:\n",
    "                    c[:3] = y[i]\n",
    "                else:\n",
    "                    c[i] = 0 if c[i] == 1 else 1   # opposite value\n",
    "            fixed_c_list.append(self.to_var(fixed_c, volatile=True))\n",
    "\n",
    "        # multi-attribute transfer (H+G, H+A, G+A, H+G+A)\n",
    "        if self.dataset == 'CelebA':\n",
    "            for i in range(4):\n",
    "                fixed_c = real_c.clone()\n",
    "                for c in fixed_c:\n",
    "                    if i in [0, 1, 3]:   # Hair color to brown\n",
    "                        c[:3] = y[2] \n",
    "                    if i in [0, 2, 3]:   # Gender\n",
    "                        c[3] = 0 if c[3] == 1 else 1\n",
    "                    if i in [1, 2, 3]:   # Aged\n",
    "                        c[4] = 0 if c[4] == 1 else 1\n",
    "                fixed_c_list.append(self.to_var(fixed_c, volatile=True))\n",
    "        return fixed_c_list\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train StarGAN within a single dataset.\"\"\"\n",
    "\n",
    "        # Set dataloader\n",
    "        if self.dataset == 'CelebA':\n",
    "            self.data_loader = self.celebA_loader\n",
    "        else:\n",
    "            self.data_loader = self.rafd_loader\n",
    "\n",
    "        # The number of iterations per epoch\n",
    "        iters_per_epoch = len(self.data_loader)\n",
    "\n",
    "        fixed_x = []\n",
    "        real_c = []\n",
    "        for i, (images, labels) in enumerate(self.data_loader):\n",
    "            fixed_x.append(images)\n",
    "            real_c.append(labels)\n",
    "            if i == 3:\n",
    "                break\n",
    "\n",
    "        # Fixed inputs and target domain labels for debugging\n",
    "        fixed_x = torch.cat(fixed_x, dim=0)\n",
    "        fixed_x = self.to_var(fixed_x, volatile=True)\n",
    "        real_c = torch.cat(real_c, dim=0)\n",
    "\n",
    "        if self.dataset == 'CelebA':\n",
    "            fixed_c_list = self.make_celeb_labels(real_c)\n",
    "        elif self.dataset == 'RaFD':\n",
    "            fixed_c_list = []\n",
    "            for i in range(self.c_dim):\n",
    "                fixed_c = self.one_hot(torch.ones(fixed_x.size(0)) * i, self.c_dim)\n",
    "                fixed_c_list.append(self.to_var(fixed_c, volatile=True))\n",
    "\n",
    "        # lr cache for decaying\n",
    "        g_lr = self.g_lr\n",
    "        d_lr = self.d_lr\n",
    "\n",
    "        # Start with trained model if exists\n",
    "        if self.pretrained_model:\n",
    "            start = int(self.pretrained_model.split('_')[0])\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        # Start training\n",
    "        start_time = time.time()\n",
    "        for e in range(start, self.num_epochs):\n",
    "            for i, (real_x, real_label) in enumerate(self.data_loader):\n",
    "                \n",
    "                # Generat fake labels randomly (target domain labels)\n",
    "                rand_idx = torch.randperm(real_label.size(0))\n",
    "                fake_label = real_label[rand_idx]\n",
    "\n",
    "                if self.dataset == 'CelebA':\n",
    "                    real_c = real_label.clone()\n",
    "                    fake_c = fake_label.clone()\n",
    "                else:\n",
    "                    real_c = self.one_hot(real_label, self.c_dim)\n",
    "                    fake_c = self.one_hot(fake_label, self.c_dim)\n",
    "\n",
    "                # Convert tensor to variable\n",
    "                real_x = self.to_var(real_x)\n",
    "                real_c = self.to_var(real_c)           # input for the generator\n",
    "                fake_c = self.to_var(fake_c)\n",
    "                real_label = self.to_var(real_label)   # this is same as real_c if dataset == 'CelebA'\n",
    "                fake_label = self.to_var(fake_label)\n",
    "                \n",
    "                # ================== Train D ================== #\n",
    "\n",
    "                # Compute loss with real images\n",
    "                out_src, out_cls = self.D(real_x)\n",
    "                d_loss_real = - torch.mean(out_src)\n",
    "\n",
    "                if self.dataset == 'CelebA':\n",
    "                    d_loss_cls = F.binary_cross_entropy_with_logits(\n",
    "                        out_cls, real_label, size_average=False) / real_x.size(0)\n",
    "                else:\n",
    "                    d_loss_cls = F.cross_entropy(out_cls, real_label)\n",
    "\n",
    "                # Compute classification accuracy of the discriminator\n",
    "                if (i+1) % self.log_step == 0:\n",
    "                    accuracies = self.compute_accuracy(out_cls, real_label, self.dataset)\n",
    "                    log = [\"{:.2f}\".format(acc) for acc in accuracies.data.cpu().numpy()]\n",
    "                    if self.dataset == 'CelebA':\n",
    "                        print('Classification Acc (Black/Blond/Brown/Gender/Aged): ', end='')\n",
    "                    else:\n",
    "                        print('Classification Acc (8 emotional expressions): ', end='')\n",
    "                    print(log)\n",
    "\n",
    "                # Compute loss with fake images\n",
    "                fake_x = self.G(real_x, fake_c)\n",
    "                fake_x = Variable(fake_x.data)\n",
    "                out_src, out_cls = self.D(fake_x)\n",
    "                d_loss_fake = torch.mean(out_src)\n",
    "\n",
    "                # Backward + Optimize\n",
    "                d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls\n",
    "                self.reset_grad()\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # Compute gradient penalty\n",
    "                alpha = torch.rand(real_x.size(0), 1, 1, 1).cuda().expand_as(real_x)\n",
    "                interpolated = Variable(alpha * real_x.data + (1 - alpha) * fake_x.data, requires_grad=True)\n",
    "                out, out_cls = self.D(interpolated)\n",
    "\n",
    "                grad = torch.autograd.grad(outputs=out,\n",
    "                                           inputs=interpolated,\n",
    "                                           grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                           retain_graph=True,\n",
    "                                           create_graph=True,\n",
    "                                           only_inputs=True)[0]\n",
    "\n",
    "                grad = grad.view(grad.size(0), -1)\n",
    "                grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "                d_loss_gp = torch.mean((grad_l2norm - 1)**2)\n",
    "\n",
    "                # Backward + Optimize\n",
    "                d_loss = self.lambda_gp * d_loss_gp\n",
    "                self.reset_grad()\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # Logging\n",
    "                loss = {}\n",
    "                loss['D/loss_real'] = d_loss_real.data[0]\n",
    "                loss['D/loss_fake'] = d_loss_fake.data[0]\n",
    "                loss['D/loss_cls'] = d_loss_cls.data[0]\n",
    "                loss['D/loss_gp'] = d_loss_gp.data[0]\n",
    "\n",
    "                # ================== Train G ================== #\n",
    "                if (i+1) % self.d_train_repeat == 0:\n",
    "\n",
    "                    # Original-to-target and target-to-original domain\n",
    "                    fake_x = self.G(real_x, fake_c)\n",
    "                    rec_x = self.G(fake_x, real_c)\n",
    "\n",
    "                    # Compute losses\n",
    "                    out_src, out_cls = self.D(fake_x)\n",
    "                    g_loss_fake = - torch.mean(out_src)\n",
    "                    g_loss_rec = torch.mean(torch.abs(real_x - rec_x))\n",
    "\n",
    "                    if self.dataset == 'CelebA':\n",
    "                        g_loss_cls = F.binary_cross_entropy_with_logits(\n",
    "                            out_cls, fake_label, size_average=False) / fake_x.size(0)\n",
    "                    else:\n",
    "                        g_loss_cls = F.cross_entropy(out_cls, fake_label)\n",
    "\n",
    "                    # Backward + Optimize\n",
    "                    g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls\n",
    "                    self.reset_grad()\n",
    "                    g_loss.backward()\n",
    "                    self.g_optimizer.step()\n",
    "\n",
    "                    # Logging\n",
    "                    loss['G/loss_fake'] = g_loss_fake.data[0]\n",
    "                    loss['G/loss_rec'] = g_loss_rec.data[0]\n",
    "                    loss['G/loss_cls'] = g_loss_cls.data[0]\n",
    "\n",
    "                # Print out log info\n",
    "                if (i+1) % self.log_step == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "\n",
    "                    log = \"Elapsed [{}], Epoch [{}/{}], Iter [{}/{}]\".format(\n",
    "                        elapsed, e+1, self.num_epochs, i+1, iters_per_epoch)\n",
    "\n",
    "                    for tag, value in loss.items():\n",
    "                        log += \", {}: {:.4f}\".format(tag, value)\n",
    "                    print(log)\n",
    "\n",
    "                    if self.use_tensorboard:\n",
    "                        for tag, value in loss.items():\n",
    "                            self.logger.scalar_summary(tag, value, e * iters_per_epoch + i + 1)\n",
    "\n",
    "                # Translate fixed images for debugging\n",
    "                if (i+1) % self.sample_step == 0:\n",
    "                    fake_image_list = [fixed_x]\n",
    "                    for fixed_c in fixed_c_list:\n",
    "                        fake_image_list.append(self.G(fixed_x, fixed_c))\n",
    "                    fake_images = torch.cat(fake_image_list, dim=3)\n",
    "                    save_image(self.denorm(fake_images.data.cpu()),\n",
    "                        os.path.join(self.sample_path, '{}_{}_fake.png'.format(e+1, i+1)),nrow=1, padding=0)\n",
    "                    print('Translated images and saved into {}..!'.format(self.sample_path))\n",
    "\n",
    "                # Save model checkpoints\n",
    "                if (i+1) % self.model_save_step == 0:\n",
    "                    torch.save(self.G.state_dict(),\n",
    "                        os.path.join(self.model_save_path, '{}_{}_G.pth'.format(e+1, i+1)))\n",
    "                    torch.save(self.D.state_dict(),\n",
    "                        os.path.join(self.model_save_path, '{}_{}_D.pth'.format(e+1, i+1)))\n",
    "\n",
    "            # Decay learning rate\n",
    "            if (e+1) > (self.num_epochs - self.num_epochs_decay):\n",
    "                g_lr -= (self.g_lr / float(self.num_epochs_decay))\n",
    "                d_lr -= (self.d_lr / float(self.num_epochs_decay))\n",
    "                self.update_lr(g_lr, d_lr)\n",
    "                print ('Decay learning rate to g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n",
    "\n",
    "    def train_multi(self):\n",
    "        \"\"\"Train StarGAN with multiple datasets.\n",
    "        In the code below, 1 is related to CelebA and 2 is releated to RaFD.\n",
    "        \"\"\"\n",
    "        # Fixed imagse and labels for debugging\n",
    "        fixed_x = []\n",
    "        real_c = []\n",
    "\n",
    "        for i, (images, labels) in enumerate(self.celebA_loader):\n",
    "            fixed_x.append(images)\n",
    "            real_c.append(labels)\n",
    "            if i == 2:\n",
    "                break\n",
    "\n",
    "        fixed_x = torch.cat(fixed_x, dim=0)\n",
    "        fixed_x = self.to_var(fixed_x, volatile=True)\n",
    "        real_c = torch.cat(real_c, dim=0)\n",
    "        fixed_c1_list = self.make_celeb_labels(real_c)\n",
    "\n",
    "        fixed_c2_list = []\n",
    "        for i in range(self.c2_dim):\n",
    "            fixed_c = self.one_hot(torch.ones(fixed_x.size(0)) * i, self.c2_dim)\n",
    "            fixed_c2_list.append(self.to_var(fixed_c, volatile=True))\n",
    "\n",
    "        fixed_zero1 = self.to_var(torch.zeros(fixed_x.size(0), self.c2_dim))     # zero vector when training with CelebA\n",
    "        fixed_mask1 = self.to_var(self.one_hot(torch.zeros(fixed_x.size(0)), 2)) # mask vector: [1, 0]\n",
    "        fixed_zero2 = self.to_var(torch.zeros(fixed_x.size(0), self.c_dim))      # zero vector when training with RaFD\n",
    "        fixed_mask2 = self.to_var(self.one_hot(torch.ones(fixed_x.size(0)), 2))  # mask vector: [0, 1]\n",
    "\n",
    "        # lr cache for decaying\n",
    "        g_lr = self.g_lr\n",
    "        d_lr = self.d_lr\n",
    "\n",
    "        # data iterator\n",
    "        data_iter1 = iter(self.celebA_loader)\n",
    "        data_iter2 = iter(self.rafd_loader)\n",
    "\n",
    "        # Start with trained model\n",
    "        if self.pretrained_model:\n",
    "            start = int(self.pretrained_model) + 1\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        # # Start training\n",
    "        start_time = time.time()\n",
    "        for i in range(start, self.num_iters):\n",
    "\n",
    "            # Fetch mini-batch images and labels\n",
    "            try:\n",
    "                real_x1, real_label1 = next(data_iter1)\n",
    "            except:\n",
    "                data_iter1 = iter(self.celebA_loader)\n",
    "                real_x1, real_label1 = next(data_iter1)\n",
    "\n",
    "            try:\n",
    "                real_x2, real_label2 = next(data_iter2)\n",
    "            except:\n",
    "                data_iter2 = iter(self.rafd_loader)\n",
    "                real_x2, real_label2 = next(data_iter2)\n",
    "\n",
    "            # Generate fake labels randomly (target domain labels)\n",
    "            rand_idx = torch.randperm(real_label1.size(0))\n",
    "            fake_label1 = real_label1[rand_idx]\n",
    "            rand_idx = torch.randperm(real_label2.size(0))\n",
    "            fake_label2 = real_label2[rand_idx]\n",
    "\n",
    "            real_c1 = real_label1.clone()\n",
    "            fake_c1 = fake_label1.clone()\n",
    "            zero1 = torch.zeros(real_x1.size(0), self.c2_dim)\n",
    "            mask1 = self.one_hot(torch.zeros(real_x1.size(0)), 2)\n",
    "\n",
    "            real_c2 = self.one_hot(real_label2, self.c2_dim)\n",
    "            fake_c2 = self.one_hot(fake_label2, self.c2_dim)\n",
    "            zero2 = torch.zeros(real_x2.size(0), self.c_dim)\n",
    "            mask2 = self.one_hot(torch.ones(real_x2.size(0)), 2)\n",
    "\n",
    "            # Convert tensor to variable\n",
    "            real_x1 = self.to_var(real_x1)\n",
    "            real_c1 = self.to_var(real_c1)\n",
    "            fake_c1 = self.to_var(fake_c1)\n",
    "            mask1 = self.to_var(mask1)\n",
    "            zero1 = self.to_var(zero1)\n",
    "\n",
    "            real_x2 = self.to_var(real_x2)\n",
    "            real_c2 = self.to_var(real_c2)\n",
    "            fake_c2 = self.to_var(fake_c2)\n",
    "            mask2 = self.to_var(mask2)\n",
    "            zero2 = self.to_var(zero2)\n",
    "\n",
    "            real_label1 = self.to_var(real_label1)\n",
    "            fake_label1 = self.to_var(fake_label1)\n",
    "            real_label2 = self.to_var(real_label2)\n",
    "            fake_label2 = self.to_var(fake_label2)\n",
    "\n",
    "            # ================== Train D ================== #\n",
    "\n",
    "            # Real images (CelebA)\n",
    "            out_real, out_cls = self.D(real_x1)\n",
    "            out_cls1 = out_cls[:, :self.c_dim]      # celebA part\n",
    "            d_loss_real = - torch.mean(out_real)\n",
    "            d_loss_cls = F.binary_cross_entropy_with_logits(out_cls1, real_label1, size_average=False) / real_x1.size(0)\n",
    "\n",
    "            # Real images (RaFD)\n",
    "            out_real, out_cls = self.D(real_x2)\n",
    "            out_cls2 = out_cls[:, self.c_dim:]      # rafd part\n",
    "            d_loss_real += - torch.mean(out_real)\n",
    "            d_loss_cls += F.cross_entropy(out_cls2, real_label2)\n",
    "\n",
    "            # Compute classification accuracy of the discriminator\n",
    "            if (i+1) % self.log_step == 0:\n",
    "                accuracies = self.compute_accuracy(out_cls1, real_label1, 'CelebA')\n",
    "                log = [\"{:.2f}\".format(acc) for acc in accuracies.data.cpu().numpy()]\n",
    "                print('Classification Acc (Black/Blond/Brown/Gender/Aged): ', end='')\n",
    "                print(log)\n",
    "                accuracies = self.compute_accuracy(out_cls2, real_label2, 'RaFD')\n",
    "                log = [\"{:.2f}\".format(acc) for acc in accuracies.data.cpu().numpy()]\n",
    "                print('Classification Acc (8 emotional expressions): ', end='')\n",
    "                print(log)\n",
    "\n",
    "            # Fake images (CelebA)\n",
    "            fake_c = torch.cat([fake_c1, zero1, mask1], dim=1)\n",
    "            fake_x1 = self.G(real_x1, fake_c)\n",
    "            fake_x1 = Variable(fake_x1.data)\n",
    "            out_fake, _ = self.D(fake_x1)\n",
    "            d_loss_fake = torch.mean(out_fake)\n",
    "\n",
    "            # Fake images (RaFD)\n",
    "            fake_c = torch.cat([zero2, fake_c2, mask2], dim=1)\n",
    "            fake_x2 = self.G(real_x2, fake_c)\n",
    "            out_fake, _ = self.D(fake_x2)\n",
    "            d_loss_fake += torch.mean(out_fake)\n",
    "\n",
    "            # Backward + Optimize\n",
    "            d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls\n",
    "            self.reset_grad()\n",
    "            d_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "\n",
    "            # Compute gradient penalty\n",
    "            if (i+1) % 2 == 0:\n",
    "                real_x = real_x1\n",
    "                fake_x = fake_x1\n",
    "            else:\n",
    "                real_x = real_x2\n",
    "                fake_x = fake_x2\n",
    "\n",
    "            alpha = torch.rand(real_x.size(0), 1, 1, 1).cuda().expand_as(real_x)\n",
    "            interpolated = Variable(alpha * real_x.data + (1 - alpha) * fake_x.data, requires_grad=True)\n",
    "            out, out_cls = self.D(interpolated)\n",
    "\n",
    "            if (i+1) % 2 == 0:\n",
    "                out_cls = out_cls[:, :self.c_dim]  # CelebA\n",
    "            else:\n",
    "                out_cls = out_cls[:, self.c_dim:]  # RaFD\n",
    "\n",
    "            grad = torch.autograd.grad(outputs=out,\n",
    "                                       inputs=interpolated,\n",
    "                                       grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                       retain_graph=True,\n",
    "                                       create_graph=True,\n",
    "                                       only_inputs=True)[0]\n",
    "\n",
    "            grad = grad.view(grad.size(0), -1)\n",
    "            grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "            d_loss_gp = torch.mean((grad_l2norm - 1)**2)\n",
    "\n",
    "            # Backward + Optimize\n",
    "            d_loss = self.lambda_gp * d_loss_gp\n",
    "            self.reset_grad()\n",
    "            d_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "\n",
    "            # Logging\n",
    "            loss = {}\n",
    "            loss['D/loss_real'] = d_loss_real.data[0]\n",
    "            loss['D/loss_fake'] = d_loss_fake.data[0]\n",
    "            loss['D/loss_cls'] = d_loss_cls.data[0]\n",
    "            loss['D/loss_gp'] = d_loss_gp.data[0]\n",
    "\n",
    "            # ================== Train G ================== #\n",
    "            if (i+1) % self.d_train_repeat == 0:\n",
    "                # Original-to-target and target-to-original domain (CelebA)\n",
    "                fake_c = torch.cat([fake_c1, zero1, mask1], dim=1)\n",
    "                real_c = torch.cat([real_c1, zero1, mask1], dim=1)\n",
    "                fake_x1 = self.G(real_x1, fake_c)\n",
    "                rec_x1 = self.G(fake_x1, real_c)\n",
    "\n",
    "                # Compute losses\n",
    "                out, out_cls = self.D(fake_x1)\n",
    "                out_cls1 = out_cls[:, :self.c_dim]\n",
    "                g_loss_fake = - torch.mean(out)\n",
    "                g_loss_rec = torch.mean(torch.abs(real_x1 - rec_x1))\n",
    "                g_loss_cls = F.binary_cross_entropy_with_logits(out_cls1, fake_label1, size_average=False) / fake_x1.size(0)\n",
    "\n",
    "                # Original-to-target and target-to-original domain (RaFD)\n",
    "                fake_c = torch.cat([zero2, fake_c2, mask2], dim=1)\n",
    "                real_c = torch.cat([zero2, real_c2, mask2], dim=1)\n",
    "                fake_x2 = self.G(real_x2, fake_c)\n",
    "                rec_x2 = self.G(fake_x2, real_c)\n",
    "\n",
    "                # Compute losses\n",
    "                out, out_cls = self.D(fake_x2)\n",
    "                out_cls2 = out_cls[:, self.c_dim:]\n",
    "                g_loss_fake += - torch.mean(out)\n",
    "                g_loss_rec += torch.mean(torch.abs(real_x2 - rec_x2))\n",
    "                g_loss_cls += F.cross_entropy(out_cls2, fake_label2)\n",
    "\n",
    "                # Backward + Optimize\n",
    "                g_loss = g_loss_fake + self.lambda_cls * g_loss_cls + self.lambda_rec * g_loss_rec\n",
    "                self.reset_grad()\n",
    "                g_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "\n",
    "                # Logging\n",
    "                loss['G/loss_fake'] = g_loss_fake.data[0]\n",
    "                loss['G/loss_cls'] = g_loss_cls.data[0]\n",
    "                loss['G/loss_rec'] = g_loss_rec.data[0]\n",
    "\n",
    "            # Print out log info\n",
    "            if (i+1) % self.log_step == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "\n",
    "                log = \"Elapsed [{}], Iter [{}/{}]\".format(\n",
    "                    elapsed, i+1, self.num_iters)\n",
    "\n",
    "                for tag, value in loss.items():\n",
    "                    log += \", {}: {:.4f}\".format(tag, value)\n",
    "                print(log)\n",
    "\n",
    "                if self.use_tensorboard:\n",
    "                    for tag, value in loss.items():\n",
    "                        self.logger.scalar_summary(tag, value, i+1)\n",
    "\n",
    "            # Translate the images (debugging)\n",
    "            if (i+1) % self.sample_step == 0:\n",
    "                fake_image_list = [fixed_x]\n",
    "\n",
    "                # Changing hair color, gender, and age\n",
    "                for j in range(self.c_dim):\n",
    "                    fake_c = torch.cat([fixed_c1_list[j], fixed_zero1, fixed_mask1], dim=1)\n",
    "                    fake_image_list.append(self.G(fixed_x, fake_c))\n",
    "                # Changing emotional expressions\n",
    "                for j in range(self.c2_dim):\n",
    "                    fake_c = torch.cat([fixed_zero2, fixed_c2_list[j], fixed_mask2], dim=1)\n",
    "                    fake_image_list.append(self.G(fixed_x, fake_c))\n",
    "                fake = torch.cat(fake_image_list, dim=3)\n",
    "\n",
    "                # Save the translated images\n",
    "                save_image(self.denorm(fake.data.cpu()),\n",
    "                    os.path.join(self.sample_path, '{}_fake.png'.format(i+1)), nrow=1, padding=0)\n",
    "\n",
    "            # Save model checkpoints\n",
    "            if (i+1) % self.model_save_step == 0:\n",
    "                torch.save(self.G.state_dict(),\n",
    "                    os.path.join(self.model_save_path, '{}_G.pth'.format(i+1)))\n",
    "                torch.save(self.D.state_dict(),\n",
    "                    os.path.join(self.model_save_path, '{}_D.pth'.format(i+1)))\n",
    "\n",
    "            # Decay learning rate\n",
    "            decay_step = 1000\n",
    "            if (i+1) > (self.num_iters - self.num_iters_decay) and (i+1) % decay_step==0:\n",
    "                g_lr -= (self.g_lr / float(self.num_iters_decay) * decay_step)\n",
    "                d_lr -= (self.d_lr / float(self.num_iters_decay) * decay_step)\n",
    "                self.update_lr(g_lr, d_lr)\n",
    "                print ('Decay learning rate to g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Facial attribute transfer on CelebA or facial expression synthesis on RaFD.\"\"\"\n",
    "        # Load trained parameters\n",
    "        G_path = os.path.join(self.model_save_path, '{}_G.pth'.format(self.test_model))\n",
    "        self.G.load_state_dict(torch.load(G_path))\n",
    "        self.G.eval()\n",
    "\n",
    "        if self.dataset == 'CelebA':\n",
    "            data_loader = self.celebA_loader\n",
    "        else:\n",
    "            data_loader = self.rafd_loader\n",
    "\n",
    "        for i, (real_x, org_c) in enumerate(data_loader):\n",
    "            real_x = self.to_var(real_x, volatile=True)\n",
    "\n",
    "            if self.dataset == 'CelebA':\n",
    "                target_c_list = self.make_celeb_labels(org_c)\n",
    "            else:\n",
    "                target_c_list = []\n",
    "                for j in range(self.c_dim):\n",
    "                    target_c = self.one_hot(torch.ones(real_x.size(0)) * j, self.c_dim)\n",
    "                    target_c_list.append(self.to_var(target_c, volatile=True))\n",
    "\n",
    "            # Start translations\n",
    "            fake_image_list = [real_x]\n",
    "            for target_c in target_c_list:\n",
    "                fake_image_list.append(self.G(real_x, target_c))\n",
    "            fake_images = torch.cat(fake_image_list, dim=3)\n",
    "            save_path = os.path.join(self.result_path, '{}_fake.png'.format(i+1))\n",
    "            save_image(self.denorm(fake_images.data), save_path, nrow=1, padding=0)\n",
    "            print('Translated test images and saved into \"{}\"..!'.format(save_path))\n",
    "\n",
    "    def test_multi(self):\n",
    "        \"\"\"Facial attribute transfer and expression synthesis on CelebA.\"\"\"\n",
    "        # Load trained parameters\n",
    "        G_path = os.path.join(self.model_save_path, '{}_G.pth'.format(self.test_model))\n",
    "        self.G.load_state_dict(torch.load(G_path))\n",
    "        self.G.eval()\n",
    "\n",
    "        for i, (real_x, org_c) in enumerate(self.celebA_loader):\n",
    "\n",
    "            # Prepare input images and target domain labels\n",
    "            real_x = self.to_var(real_x, volatile=True)\n",
    "            target_c1_list = self.make_celeb_labels(org_c)\n",
    "            target_c2_list = []\n",
    "            for j in range(self.c2_dim):\n",
    "                target_c = self.one_hot(torch.ones(real_x.size(0)) * j, self.c2_dim)\n",
    "                target_c2_list.append(self.to_var(target_c, volatile=True))\n",
    "\n",
    "            # Zero vectors and mask vectors\n",
    "            zero1 = self.to_var(torch.zeros(real_x.size(0), self.c2_dim))     # zero vector for rafd expressions\n",
    "            mask1 = self.to_var(self.one_hot(torch.zeros(real_x.size(0)), 2)) # mask vector: [1, 0]\n",
    "            zero2 = self.to_var(torch.zeros(real_x.size(0), self.c_dim))      # zero vector for celebA attributes\n",
    "            mask2 = self.to_var(self.one_hot(torch.ones(real_x.size(0)), 2))  # mask vector: [0, 1]\n",
    "\n",
    "            # Changing hair color, gender, and age\n",
    "            fake_image_list = [real_x]\n",
    "            for j in range(self.c_dim):\n",
    "                target_c = torch.cat([target_c1_list[j], zero1, mask1], dim=1)\n",
    "                fake_image_list.append(self.G(real_x, target_c))\n",
    "\n",
    "            # Changing emotional expressions\n",
    "            for j in range(self.c2_dim):\n",
    "                target_c = torch.cat([zero2, target_c2_list[j], mask2], dim=1)\n",
    "                fake_image_list.append(self.G(real_x, target_c))\n",
    "            fake_images = torch.cat(fake_image_list, dim=3)\n",
    "\n",
    "            # Save the translated images\n",
    "            save_path = os.path.join(self.result_path, '{}_fake.png'.format(i+1))\n",
    "            save_image(self.denorm(fake_images.data), save_path, nrow=1, padding=0)\n",
    "            print('Translated test images and saved into \"{}\"..!'.format(save_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true,
    "hide_input": false
   },
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block.\"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator. Encoder-Decoder Architecture.\"\"\"\n",
    "    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # Down-Sampling\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(2):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        # Bottleneck\n",
    "        for i in range(repeat_num):\n",
    "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
    "\n",
    "        # Up-Sampling\n",
    "        for i in range(2):\n",
    "            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim // 2\n",
    "\n",
    "        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # replicate spatially and concatenate domain information\n",
    "        c = c.unsqueeze(2).unsqueeze(3)\n",
    "        c = c.expand(c.size(0), c.size(1), x.size(2), x.size(3))\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator. PatchGAN.\"\"\"\n",
    "    def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.LeakyReLU(0.01, inplace=True))\n",
    "\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(1, repeat_num):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.01, inplace=True))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        k_size = int(image_size / np.power(2, repeat_num))\n",
    "        self.main = nn.Sequential(*layers)\n",
    "        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=k_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.main(x)\n",
    "        out_real = self.conv1(h)\n",
    "        out_aux = self.conv2(h)\n",
    "        return out_real.squeeze(), out_aux.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "def main(config):\n",
    "    # For fast training\n",
    "    cudnn.benchmark = True\n",
    "    # Create directories if not exist\n",
    "    if not os.path.exists(config.log_path):\n",
    "        os.makedirs(config.log_path)\n",
    "    if not os.path.exists(config.model_save_path):\n",
    "        os.makedirs(config.model_save_path)\n",
    "    if not os.path.exists(config.sample_path):\n",
    "        os.makedirs(config.sample_path)\n",
    "    if not os.path.exists(config.result_path):\n",
    "        os.makedirs(config.result_path)\n",
    "    # Data loader\n",
    "    celebA_loader = None\n",
    "    rafd_loader = None\n",
    "\n",
    "    if config.dataset in ['CelebA', 'Both']:\n",
    "        celebA_loader = get_loader(config.celebA_image_path, config.metadata_path, config.celebA_crop_size,\n",
    "                                   config.image_size, config.batch_size, 'CelebA', config.mode)\n",
    "    if config.dataset in ['RaFD', 'Both']:\n",
    "        rafd_loader = get_loader(config.rafd_image_path, None, config.rafd_crop_size,\n",
    "                                 config.image_size, config.batch_size, 'RaFD', config.mode)\n",
    "\n",
    "    # Solver\n",
    "    solver = Solver(celebA_loader, rafd_loader, config)\n",
    "\n",
    "    if config.mode == 'train':\n",
    "        if config.dataset in ['CelebA', 'RaFD']:\n",
    "            solver.train()\n",
    "        elif config.dataset in ['Both']:\n",
    "            solver.train_multi()\n",
    "    elif config.mode == 'test':\n",
    "        if config.dataset in ['CelebA', 'RaFD']:\n",
    "            solver.test()\n",
    "        elif config.dataset in ['Both']:\n",
    "            solver.test_multi()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Model hyper-parameters\n",
    "    parser.add_argument('--c_dim', type=int, default=5)\n",
    "    parser.add_argument('--c2_dim', type=int, default=8)\n",
    "    parser.add_argument('--celebA_crop_size', type=int, default=178)\n",
    "    parser.add_argument('--rafd_crop_size', type=int, default=256)\n",
    "    parser.add_argument('--image_size', type=int, default=128)\n",
    "    parser.add_argument('--g_conv_dim', type=int, default=64)\n",
    "    parser.add_argument('--d_conv_dim', type=int, default=64)\n",
    "    parser.add_argument('--g_repeat_num', type=int, default=6)\n",
    "    parser.add_argument('--d_repeat_num', type=int, default=6)\n",
    "    parser.add_argument('--g_lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--d_lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--lambda_cls', type=float, default=1)\n",
    "    parser.add_argument('--lambda_rec', type=float, default=10)\n",
    "    parser.add_argument('--lambda_gp', type=float, default=10)\n",
    "    parser.add_argument('--d_train_repeat', type=int, default=5)\n",
    "\n",
    "    # Training settings\n",
    "    parser.add_argument('--dataset', type=str, default='CelebA', choices=['CelebA', 'RaFD', 'Both'])\n",
    "    parser.add_argument('--num_epochs', type=int, default=20)\n",
    "    parser.add_argument('--num_epochs_decay', type=int, default=10)\n",
    "    parser.add_argument('--num_iters', type=int, default=200000)\n",
    "    parser.add_argument('--num_iters_decay', type=int, default=100000)\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    parser.add_argument('--num_workers', type=int, default=1)\n",
    "    parser.add_argument('--beta1', type=float, default=0.5)\n",
    "    parser.add_argument('--beta2', type=float, default=0.999)\n",
    "    parser.add_argument('--pretrained_model', type=str, default=None)\n",
    "\n",
    "    # Test settings\n",
    "    parser.add_argument('--test_model', type=str, default='20_1000')\n",
    "\n",
    "    # Misc\n",
    "    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
    "    parser.add_argument('--use_tensorboard', type=str2bool, default=False)\n",
    "\n",
    "    # Path\n",
    "    parser.add_argument('--celebA_image_path', type=str, default='./data/CelebA_nocrop/images')\n",
    "    parser.add_argument('--rafd_image_path', type=str, default='./data/RaFD/train')\n",
    "    parser.add_argument('--metadata_path', type=str, default='./data/list_attr_celeba.txt')\n",
    "    parser.add_argument('--log_path', type=str, default='./stargan/logs')\n",
    "    parser.add_argument('--model_save_path', type=str, default='./stargan/models')\n",
    "    parser.add_argument('--sample_path', type=str, default='./stargan/samples')\n",
    "    parser.add_argument('--result_path', type=str, default='./stargan/results')\n",
    "\n",
    "    # Step size\n",
    "    parser.add_argument('--log_step', type=int, default=10)\n",
    "    parser.add_argument('--sample_step', type=int, default=500)\n",
    "    parser.add_argument('--model_save_step', type=int, default=1000)\n",
    "\n",
    "    config = parser.parse_args()\n",
    "    print(config)\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using a trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
